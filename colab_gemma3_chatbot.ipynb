{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjpf4dv7IvdCWIsROIX5CU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boysbytes/colab-chatbot/blob/main/colab_gemma3_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Give access to your Google Drive\n",
        "\n",
        "Allow the notebook to access your Google Drive files and set up the `ollama_models` directory.\n",
        "\n",
        "This directory will contain the models you download later."
      ],
      "metadata": {
        "id": "TGuHTQAAZjdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import gradio as gr\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create ollama_models folder if it doesn't exist\n",
        "ollama_models_path = '/content/drive/MyDrive/ollama_models'\n",
        "if not os.path.exists(ollama_models_path):\n",
        "    os.makedirs(ollama_models_path)\n",
        "    print(f\"Created directory: {ollama_models_path}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {ollama_models_path}\")"
      ],
      "metadata": {
        "id": "M9rk4XFQtuZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "RcNFy8hMR7Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Install libraries\n",
        "!pip install ollama gradio\n",
        "\n",
        "# Start Ollama in a separate process\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)\n",
        "\n",
        "running = subprocess.run([\"pgrep\", \"-a\", \"ollama\"], capture_output=True, text=True)\n",
        "print(\"Ollama Status:\", running.stdout if running.stdout else \"Ollama is NOT running!\")\n",
        "\n",
        "# Create symlink\n",
        "!rm -rf ~/.ollama/models\n",
        "!ln -s /content/drive/MyDrive/ollama_models ~/.ollama/models\n"
      ],
      "metadata": {
        "id": "R2C4RvcTZmMt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the model to Google Drive\n",
        "\n",
        "To change the model:\n",
        "1. Find a model at [Ollama](https://ollama.com/search).\n",
        "\n",
        "  Use a small model because you don't want to exceed the storage space in your Google Drive storage space and the compute resources in Google Colab.\n",
        "\n",
        "2. Modify this line and replace `gemma3:1b` with the new model:\n",
        "\n",
        "  ```bash\n",
        "  !ollama pull gemma3:1b\n",
        "  ```\n",
        "\n",
        "3. After the model is downloaded, go to Google Drive and note the path of the model.\n",
        "\n",
        "  Update the model path. This ensures the model won't be redownloaded the next time you run this notebook.\n",
        "\n",
        "  ```bash\n",
        "  model_info_path = '/content/drive/MyDrive/ollama_models/manifests/registry.ollama.ai/library/gemini3/1b'\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "mjQ2hiXBbDek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "model_info_path = '/content/drive/MyDrive/ollama_models/manifests/registry.ollama.ai/library/gemma3/1b'\n",
        "if not os.path.exists(model_info_path):\n",
        "    !ollama pull gemma3:1b\n",
        "else:\n",
        "    print(\"Model already exists. Skipping download.\")"
      ],
      "metadata": {
        "id": "Z4szQvG6bE5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the chatbot function"
      ],
      "metadata": {
        "id": "cRl-Q6iKW6j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = [] #Initialize outside of the Blocks\n",
        "\n",
        "def generate(prompt, context, model, temperature, num_predict):\n",
        "    \"\"\"Call the Ollama API to generate a response.\"\"\"\n",
        "    api_response = requests.post(\n",
        "        'http://localhost:11434/api/generate',\n",
        "        json={\n",
        "            'model': model,\n",
        "            'prompt': prompt,\n",
        "            'context': context,\n",
        "            'options': {\n",
        "                'temperature': temperature,\n",
        "                'num_predict': num_predict\n",
        "            }\n",
        "        },\n",
        "        stream=False\n",
        "    )\n",
        "    api_response.raise_for_status()\n",
        "\n",
        "    response = \"\"\n",
        "    for line in api_response.iter_lines():\n",
        "        body = json.loads(line)\n",
        "        response_part = body.get('response', '')\n",
        "        if 'error' in body:\n",
        "            raise Exception(body['error'])\n",
        "        response += response_part\n",
        "        if body.get('done', False):\n",
        "            context = body.get('context', [])\n",
        "    return response, context\n",
        "\n",
        "def chatbot_response(user_message, chat_history, user_prompt, temperature, max_tokens):\n",
        "    \"\"\"Generate chatbot responses dynamically.\"\"\"\n",
        "    global context\n",
        "\n",
        "    # Format conversation history and prompt template with fixed structure\n",
        "    chat_history_formatted = \"\\n\".join([f\"{role}: {msg}\" for role, msg in chat_history])\n",
        "\n",
        "    # Fixed prompt template with user-defined prompt at the top\n",
        "    prompt_template = f\"\"\"{user_prompt}\n",
        "Conversation History:\n",
        "{chat_history_formatted}\n",
        "User's Message: {user_message}\n",
        "Chatbot's Response:\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Generate response using Ollama API\n",
        "        response, context = generate(prompt_template, context, model=\"gemma3:1b\", temperature=temperature, num_predict=max_tokens)\n",
        "        bot_message = response.strip()\n",
        "    except Exception as e:\n",
        "        bot_message = f\"Error: {e}\"\n",
        "\n",
        "    # Update chat history\n",
        "    chat_history.append((\"User\", user_message))\n",
        "    chat_history.append((\"Chatbot\", bot_message))\n",
        "\n",
        "    return chat_history, chat_history"
      ],
      "metadata": {
        "id": "L_v3QojxXBjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the chatbot interface"
      ],
      "metadata": {
        "id": "TtQLB7dmXLz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Gradio interface with fixed dimensions using CSS styling\n",
        "DEFAULT_USER_PROMPT = \"You are a helpful assistant.\"\n",
        "\n",
        "with gr.Blocks(css=\".container {width: 1280px; height: 350px;} .compact-textbox {padding: 0; margin: 0; height: 150px;} .chatbot {max-height: 200px; overflow-y: auto;} .message-box textarea {height: 30px !important; min-height: 30px !important; max-height: 30px !important;}\") as demo:\n",
        "    gr.Markdown(\"# ðŸ¤– Chatbot (Google Colab)\")\n",
        "\n",
        "    # Initialize chat history and context\n",
        "    chat_history = gr.State([])\n",
        "    context = []  # Reset context for memory\n",
        "\n",
        "    def chatbot_response(user_message, chat_history, user_prompt, temperature, max_tokens):\n",
        "        \"\"\"Generate chatbot responses dynamically.\"\"\"\n",
        "        global context\n",
        "\n",
        "        # Format conversation history and prompt template with fixed structure\n",
        "        chat_history_formatted = \"\\n\".join([f\"{role}: {msg}\" for role, msg in chat_history])\n",
        "\n",
        "        # Fixed prompt template with user-defined prompt at the top\n",
        "        prompt_template = f\"\"\"{user_prompt}\n",
        "Conversation History:\n",
        "{chat_history_formatted}\n",
        "User's Message: {user_message}\n",
        "Chatbot's Response:\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Generate response using Ollama API\n",
        "            response, context = generate(prompt_template, context, model=\"gemma3:1b\", temperature=temperature, num_predict=max_tokens)\n",
        "            bot_message = response.strip()\n",
        "        except Exception as e:\n",
        "            bot_message = f\"Error: {e}\"\n",
        "\n",
        "        # Update chat history\n",
        "        chat_history.append((\"User\", user_message))\n",
        "        chat_history.append((\"Chatbot\", bot_message))\n",
        "\n",
        "        return chat_history, chat_history\n",
        "\n",
        "    def clear_chat():\n",
        "        \"\"\"Clear the chat history and reset context.\"\"\"\n",
        "        global context\n",
        "        context = []  # Reset context for memory\n",
        "        return [], []  # Clear chat history\n",
        "\n",
        "    with gr.Row():\n",
        "        # Left Column for Prompt and Sliders\n",
        "        with gr.Column(scale=1):\n",
        "            user_prompt_input = gr.Textbox(\n",
        "                value=DEFAULT_USER_PROMPT,\n",
        "                lines=5,\n",
        "                max_lines=5,\n",
        "                label=None,\n",
        "                info=\"Modify this prompt to change the chatbot's persona.\",\n",
        "                elem_classes=[\"compact-textbox\"]\n",
        "            )\n",
        "\n",
        "            temperature_slider = gr.Slider(\n",
        "                minimum=0.0,\n",
        "                maximum=1.0,\n",
        "                step=0.1,\n",
        "                value=0.7,\n",
        "                label=\"Temperature\",\n",
        "                info=\"Adjust the randomness of the chatbot's responses.\"\n",
        "            )\n",
        "\n",
        "            max_tokens_slider = gr.Slider(\n",
        "                minimum=50,\n",
        "                maximum=500,\n",
        "                step=50,\n",
        "                value=200,\n",
        "                label=\"Max Tokens\",\n",
        "                info=\"Set the maximum length of the chatbot's response.\"\n",
        "            )\n",
        "\n",
        "        # Right Column for Chatbox and Message Input\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(elem_classes=[\"chatbot\"])\n",
        "            msg_input = gr.Textbox(\n",
        "                label=\"Your Message\",\n",
        "                placeholder=\"Ask me anything!\",\n",
        "                lines=1,  # Display only one line\n",
        "                max_lines=1,  # Prevent vertical expansion\n",
        "                elem_classes=[\"message-box\"]\n",
        "            )\n",
        "            send_btn = gr.Button(\"Send\")\n",
        "            clear_btn = gr.Button(\"Clear\")  # Clear button\n",
        "\n",
        "    send_btn.click(\n",
        "        chatbot_response,\n",
        "        inputs=[msg_input, chat_history, user_prompt_input, temperature_slider, max_tokens_slider],\n",
        "        outputs=[chatbot, chat_history]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        clear_chat,\n",
        "        inputs=[],\n",
        "        outputs=[chatbot, chat_history]  # Clear both chatbot display and history\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "sc9yje18XN5J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}